import streamlit as st
import vertexai
from vertexai.generative_models import GenerativeModel
import json

# --- 1. ì„¤ì • ì •ë³´ ---
PROJECT_ID = "fenrir-ai-project"
LOCATION = "us-central1" 
ENDPOINT_ID = "5973430069916336128"
# --------------------

# --- 2. [ìµœì¢… ìˆ˜ì •] ì¸ì¦ ë° ëª¨ë“  ë¦¬ì†ŒìŠ¤ ë¡œë“œ ---
@st.cache_resource
def load_resources():
    # --- ì¸ì¦ ì •ë³´ ì²˜ë¦¬ ---
    credentials = None
    # Streamlit Cloud í™˜ê²½ì—ì„œëŠ” st.secretsë¥¼ í†µí•´ ì¸ì¦ ì •ë³´ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.
    if hasattr(st, 'secrets') and "gcp_service_account" in st.secrets:
        creds_dict = st.secrets["gcp_service_account"]
        credentials = google.oauth2.service_account.Credentials.from_service_account_info(creds_dict)
    # ë¡œì»¬ í™˜ê²½ì—ì„œëŠ” ê¸°ì¡´ì˜ gcloud auth ë°©ì‹ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.
    
    # --- ëª¨ë¸ ë¡œë“œ ---
    fenrir_model = None
    try:
        vertexai.init(project=PROJECT_ID, location=LOCATION, credentials=credentials)
        model_endpoint = f"projects/{PROJECT_ID}/locations/{LOCATION}/endpoints/{ENDPOINT_ID}"
        fenrir_model = GenerativeModel(model_endpoint)
    except Exception as e:
        st.error(f"ëª¨ë¸ ì´ˆê¸°í™” ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

    # --- ê°œë… êµê³¼ì„œ ë¡œë“œ ---
    concept_db = {}
    try:
        with open('concepts_su1.json', 'r', encoding='utf-8') as f:
            concept_db_su1 = json.load(f)
        with open('concepts_su2.json', 'r', encoding='utf-8') as f:
            concept_db_su2 = json.load(f)
        concept_db = {**concept_db_su1, **concept_db_su2}
    except Exception as e:
        st.error(f"ê°œë… êµê³¼ì„œ ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        
    return fenrir_model, concept_db

fenrir_model, concept_database = load_resources()

# --- 3. UI êµ¬ì„± ---
st.set_page_config(layout="wide")
st.title("ğŸ”¥ Fenrir AI : The Hyper-Intelligent Personal Tutor")
st.header("v3.1 - Final Architecture")

if not fenrir_model or not concept_database:
    st.error("í•µì‹¬ ë¦¬ì†ŒìŠ¤ ë¡œë”©ì— ì‹¤íŒ¨í•˜ì—¬ í”„ë¡œí† íƒ€ì…ì„ ì‹¤í–‰í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
else:
    # --- 4. UI ì‹¤í–‰ ë¡œì§ ---
    col1, col2 = st.columns(2)

    with col1:
        st.subheader("ë¬¸ì œ ë° í’€ì´ ì…ë ¥")
        problem_text_input = st.text_area("ë¶„ì„í•  ë¬¸ì œì˜ ë‚´ìš©ì„ ì…ë ¥í•˜ì„¸ìš”.", height=200)
        user_solution_input = st.text_area("ìì‹ ì˜ í’€ì´ ê³¼ì •ì„ ììœ ë¡­ê²Œ ì„œìˆ í•˜ì„¸ìš”.", height=250)

    with col2:
        st.subheader("Fenrir AI ë¶„ì„ ê²°ê³¼")

        if st.button("ë¶„ì„ ìš”ì²­"):
            if problem_text_input and user_solution_input:
                with st.spinner("Fenrir AIê°€ íšŒì¥ë‹˜ì˜ í’€ì´ë¥¼ ë¶„ì„ ì¤‘ì…ë‹ˆë‹¤... (1/2ë‹¨ê³„: ê°œë… ì¶”ì¶œ)"):
                    # --- [ìµœì¢… ìˆ˜ì •] 1ë‹¨ê³„ í”„ë¡¬í”„íŠ¸ ìˆ˜ì • ---
                    keyword_instruction = (
                        "ë‹¹ì‹ ì€ ìµœê³ ì˜ ìˆ˜ëŠ¥ ìˆ˜í•™ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ë‹¹ì‹ ì˜ ì²« ë²ˆì§¸ ì„ë¬´ëŠ” ì£¼ì–´ì§„ ë¬¸ì œì™€ í•™ìƒì˜ í’€ì´ë¥¼ ë¶„ì„í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.\n"
                        "1. ë¨¼ì €, ì£¼ì–´ì§„ ë¬¸ì œë¥¼ ë‹¹ì‹  ìŠ¤ìŠ¤ë¡œ í’€ì–´ë³´ì•„ ì–´ë–¤ ê°œë…ë“¤ì´ í•„ìš”í•œì§€ ë‚´ì ìœ¼ë¡œ íŒŒì•…í•˜ì‹œì˜¤.\n"
                        "2. ê·¸ í›„, ë‹¹ì‹ ì˜ í’€ì´ ê³¼ì •ê³¼ í•™ìƒì˜ í’€ì´ ê³¼ì •ì„ ì¢…í•©ì ìœ¼ë¡œ ê³ ë ¤í•˜ì—¬ ê´€ë ¨ëœ í•µì‹¬ ê°œë… í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•˜ì‹œì˜¤.\n"
                        "3. í‚¤ì›Œë“œëŠ” ë°˜ë“œì‹œ ë‹¤ìŒ ëª©ë¡ ì¤‘ì—ì„œë§Œ ì„ íƒí•´ì•¼ í•˜ë©°, ìµœì¢… ê²°ê³¼ëŠ” ì˜¤ì§ JSON ë¦¬ìŠ¤íŠ¸ í˜•ì‹ìœ¼ë¡œë§Œ ì¶œë ¥í•˜ì‹œì˜¤: " + ", ".join(concept_database.keys())
                    )
                    keyword_input = f"### ë¬¸ì œ:\n{problem_text_input}\n\n### í•™ìƒì˜ í’€ì´:\n{user_solution_input}"
                    keyword_prompt = str({"instruction": keyword_instruction, "input": keyword_input})
                    
                    try:
                        response_keywords_raw = fenrir_model.generate_content(keyword_prompt).text
                        cleaned_json_str = response_keywords_raw.strip().replace("```json", "").replace("```", "").strip()
                        extracted_keywords = json.loads(cleaned_json_str)
                    except Exception as e:
                        st.warning("1ë‹¨ê³„: ê°œë… í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•˜ëŠ” ë° ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ì¼ë°˜ ë¶„ì„ì„ ì‹œë„í•©ë‹ˆë‹¤.")
                        extracted_keywords = []

                with st.spinner("Fenrir AIê°€ íšŒì¥ë‹˜ì˜ í’€ì´ë¥¼ ë¶„ì„ ì¤‘ì…ë‹ˆë‹¤... (2/2ë‹¨ê³„: ìµœì¢… í”¼ë“œë°± ìƒì„±)"):
                    # --- 2ë‹¨ê³„ ë¡œì§ì€ ë³€ê²½ ì—†ìŒ ---
                    retrieved_context = ""
                    if extracted_keywords:
                        context_list = ["### ì°¸ê³  ê°œë…\n"]
                        for key in extracted_keywords:
                            if key in concept_database:
                                context_list.append(f"#### {key}\n{json.dumps(concept_database[key], ensure_ascii=False, indent=2)}\n")
                        retrieved_context = "\n".join(context_list)
                    
                    final_instruction = (
                        "ë‹¹ì‹ ì€ ì„¸ê³„ ìµœê³ ì˜ ìˆ˜ëŠ¥ ìˆ˜í•™ ê³¼ì™¸ ì„ ìƒë‹˜ 'Fenrir AI'ì…ë‹ˆë‹¤. ë‹¹ì‹ ì€ ë‹¨ìˆœí•œ ì±„ì ê¸°ê°€ ì•„ë‹ˆë¼, í•™ìƒì˜ ì‚¬ê³  ê³¼ì •ì„ ì´í•´í•˜ê³  ì„±ì¥ì„ ë•ëŠ” ì§€í˜œë¡œìš´ íŠœí„°ì…ë‹ˆë‹¤.\n"
                        "ì£¼ì–´ì§„ ë¬¸ì œì™€ í•™ìƒì˜ í’€ì´ ê³¼ì •ì„ ë³´ê³ , ë‹¤ìŒ ê·œì¹™ì— ë”°ë¼ í”¼ë“œë°±ì„ ì œê³µí•˜ì‹œì˜¤.\n\n"
                        "1. ìš°ì„  í•™ìƒì´ ì œì‹œí•œ ë¬¸ì œë¥¼ ë³´ê³  í‘œì¤€ í’€ì´ë²•ê³¼ **ìµœì í™” í’€ì´ë²•** ë‘ ê°€ì§€ë¥¼ ë‹¤ ìƒê°í•´ ë³´ì‹œì˜¤. ê·¸ ì´í›„ **ì¶œì œìê°€ ê¸°ëŒ€í•˜ëŠ” ì ‘ê·¼ë²•**ì„ ìƒê°í•˜ê³  **ì¶œì œì ì˜ë„ ë¶„ì„**ì„ í•´ì„œ í‘œì¤€ í’€ì´ë²•ì— ë¹„í•´ **ìµœì í™” í’€ì´**ê°€ ì™œ ë” ìš°ìˆ˜í•œì§€ **ì¶œì œì ì˜ë„**ë¥¼ ì¶”ì¶œí•˜ì‹œì˜¤.\n"
                        "2. í•™ìƒì˜ í’€ì´ê°€ ìµœì¢… ì •ë‹µì— ë„ë‹¬í–ˆëŠ”ì§€, ê·¸ë¦¬ê³  ê·¸ ê³¼ì •ì´ ìˆ˜í•™ì ìœ¼ë¡œ ì˜¬ë°”ë¥¸ì§€ ë…¼ë¦¬ì  ì˜¤ë¥˜ê°€ ì—†ëŠ”ì§€ í™•ì¸í•˜ì‹œì˜¤.\n"
                        "3. í•™ìƒì˜ í’€ì´ë¥¼ ê¼¼ê¼¼íˆ ê²€ì‚¬í•˜ì—¬ **ë§Œì•½ í•™ìƒì˜ í’€ì´ ê³¼ì •ì— 'ì‚¬ì†Œí•œ ì‹¤ìˆ˜'ë‚˜ ëª…ë°±í•œ 'ê³„ì‚° ì‹¤ìˆ˜'ë‚˜ 'ë…¼ë¦¬ì  ì˜¤ë¥˜'ê°€ ì¡´ì¬í•œë‹¤ë©´**, ê·¸ ë¶€ë¶„ì„ ì •í™•íˆ ì§šì–´ì£¼ê³  ì™œ í‹€ë ¸ëŠ”ì§€ ì„¤ëª…í•˜ì‹œì˜¤.\n"
                        "4. í•™ìƒì˜ í’€ì´ì™€ ë‹¹ì‹ ì˜ ë‚´ì¬ëœ ì§€ì‹(í‘œì¤€ í’€ì´, ìµœì í™” í’€ì´)ê³¼ ë¹„êµí•˜ì‹œì˜¤. **ë§Œì•½ í•™ìƒì˜ í’€ì´ê°€ ë…¼ë¦¬ì  ì˜¤ë¥˜ ì—†ì´ ì •ë‹µì„ ë§í˜”ë”ë¼ë„**,  í•™ìƒì˜ í’€ì´ê°€ ë¹„íš¨ìœ¨ì ì´ê±°ë‚˜ í‘œì¤€ í’€ì´ì— ê°€ê¹ë‹¤ê³  íŒë‹¨ë˜ë©´, í•™ìƒì˜ í’€ì´ë¥¼ ì¸ì •í•´ì£¼ë©´ì„œ **'ì¶œì œ ì˜ë„ ë¶„ì„'**ì˜ ê´€ì ì—ì„œ ë” ë‚˜ì€ ë°©ë²•(ìµœì í™” í’€ì´)ì´ ìˆìŒì„ íŒíŠ¸ì™€ í•¨ê»˜ ì œì‹œí•˜ì‹œì˜¤.\n"
                        "5. **ë§Œì•½ í•™ìƒì˜ í’€ì´ê°€ ìµœì í™” í’€ì´ì™€ ìœ ì‚¬í•œ ì™„ë²½í•œ í’€ì´ë¼ë©´**, ì¹­ì°¬ê³¼ í•¨ê»˜ í•´ë‹¹ ë¬¸ì œì˜ í•µì‹¬ ê°œë…ì„ ë‹¤ì‹œ í•œë²ˆ ì§šì–´ì£¼ì–´ ì§€ì‹ì„ ê³µê³ íˆ í•˜ë„ë¡ ë„ìš°ì‹œì˜¤.\n"
                        "6. í•™ìƒì˜ í‘œí˜„ì´ ë‹¤ì†Œ ë¶€ì •í™•í•˜ë”ë¼ë„ ì „ì²´ì ì¸ ë…¼ë¦¬ì  íë¦„ì´ ë§ê±°ë‚˜ ë…¼ë¦¬ì ìœ¼ë¡œ ë™ì¹˜ì´ë©´, 'ë…¼ë¦¬ì  ì˜¤ë¥˜'ë¡œ ì§€ì í•˜ê¸°ë³´ë‹¤ëŠ” ë” ì •í™•í•œ í‘œí˜„ì„ ì œì•ˆí•˜ëŠ” ë°©ì‹ìœ¼ë¡œ ë¶€ë“œëŸ½ê²Œ êµì •í•˜ì‹œì˜¤.\n"
                        "7. ê°œë… êµì¬ì—ì„œ ì¶”ì¶œí•œ í‚¤ì›Œë“œë“¤ì„ ê°œë…ì— ê·¼ê±°í•˜ì—¬ ìµœì í™” í’€ì´ì™€ mapping í•˜ì‹œì˜¤.\n"
                        "8. ë‹µë³€ ì‹œì‘ ë¶€ë¶„ì€ ì¹œì ˆí•œ íŠœí„°ë¡œì„œ ì¸ì‚¬ë¥¼ í•˜ê³  ê·¸ ì´í›„ ë°˜ë“œì‹œ ë‹¤ìŒì˜ 4ë‹¨ê³„ í˜•ì‹ì— ë§ì¶° ëª©ì°¨ì‹ìœ¼ë¡œ êµ¬ì„±í•˜ì‹œì˜¤: '1. ë¶„ì„ ê²°ë¡ ', '2. í•™ìƒì˜ í’€ì´ ê³¼ì • ê²€í† ', '3. ìµœì í™” í’€ì´', '4. ê°œë… Mapping', '5. ê²°ë¡ : ì¶œì œìì˜ ì˜ë„ ë¶„ì„'."
                    )
                    final_input = (f"{retrieved_context}\n\n---\n\n### ë¬¸ì œ\n{problem_text_input}\n\n### í•™ìƒì˜ í’€ì´\n{user_solution_input}")
                    final_prompt = str({"instruction": final_instruction, "input": final_input})
                    
                    try:
                        final_response = fenrir_model.generate_content(final_prompt)
                        st.session_state['analysis_result'] = final_response.text
                    except Exception as e:
                        st.error(f"API í˜¸ì¶œ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
            else:
                st.warning("ë¶„ì„ì„ ìœ„í•´ ë¬¸ì œì™€ í’€ì´ ê³¼ì •ì„ ëª¨ë‘ ì…ë ¥í•´ì£¼ì„¸ìš”.")

        if 'analysis_result' in st.session_state:
            st.markdown(st.session_state['analysis_result'])